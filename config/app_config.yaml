# EPUB Application Configuration

# サーバー設定
server:
    host: '0.0.0.0'
    port: 8000

# ディレクトリ設定
directories:
    epub_dir: 'epub'
    cache_dir: 'cache'
    log_dir: 'log'
    config_dir: 'config'

# ログ設定
logging:
    level: 'INFO'
    format: '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
    date_format: '%Y-%m-%d %H:%M:%S'
    file_encoding: 'utf-8'

    # ログファイル設定
    files:
        app_log: 'epub-app.log'
        server_log: 'server.log'

# キャッシュ設定
cache:
    text_cache: true
    cover_cache: true

# MLX 設定
mlx:
    # 使用する埋め込みモデル（mlx_lm でロード可能なID）
    # 例: 'mlx-community/multilingual-e5-large-mlx'
    embedding_model: 'mlx-community/multilingual-e5-large-mlx'

# 環境変数オーバーライド設定
env_overrides:
    # 環境変数名 -> 設定キーパスのマッピング
    SERVER_HOST: 'server.host'
    SERVER_PORT: 'server.port'
    MLX_EMBEDDING_MODEL: 'mlx.embedding_model'
    LMSTUDIO_BASE_URL: 'lmstudio.base_url'
    LMSTUDIO_MODEL: 'lmstudio.model'
    LMSTUDIO_TEMPERATURE: 'lmstudio.temperature'
    LMSTUDIO_MAX_TOKENS: 'lmstudio.max_tokens'
    LMSTUDIO_ALLOWED_MODELS: 'lmstudio.allowed_models'
    LMSTUDIO_DISALLOWED_MODELS: 'lmstudio.disallowed_models'

# LM Studio 設定
lmstudio:
    # LM Studio OpenAI 互換エンドポイント設定（YAML で管理）
    base_url: 'http://localhost:1234/v1'
    # 起動時に必須。LM Studio 側でロード済みのモデルIDを指定する。
    # 例: curl http://localhost:1234/v1/models で一覧取得可能。
    model: 'openai/gpt-oss-20b'
    temperature: 0.2
    max_tokens: 20480
    allowed_models: ['openai/gpt-oss-20b']
    disallowed_models: 'qwen3-30b-a3b-instruct-2507'

    # コンテキスト収集に関するデフォルト
    per_book_top_k: 3 # 各書籍での上位取得数
    all_books_top_k: 5 # 全書籍検索時の上位取得数
    max_context_snippets: 12 # LLM へ渡す最大スニペット数
